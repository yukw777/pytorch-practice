{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10687f258>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[e] for e in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8, 'boy': 9, 'wrote': 10, 'letter': 11}\n",
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, ' ': 52}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# simple training data\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"The boy wrote the letter\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "char_to_ix = {c: i for i, c in enumerate(string.ascii_letters + ' ')}\n",
    "print(char_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "ix_to_tag = {ix: tag for tag, ix in tag_to_ix.items()}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      "[torch.LongTensor of size 4]\n",
      "\n",
      "Variable containing:\n",
      " 1.0386  0.5206 -0.5006  1.2182  0.2117\n",
      "-1.0613 -1.9441 -0.9596  0.5489 -0.9901\n",
      "-0.3826  1.5037  1.8267  0.5561  1.6445\n",
      " 0.4973 -1.5067  1.7661 -0.3569 -0.1713\n",
      "[torch.FloatTensor of size 4x5]\n",
      "\n",
      "Variable containing:\n",
      " 1.0386  0.5206 -0.5006  1.2182  0.2117 -1.9366  1.0067 -1.8593  0.9329  1.4066\n",
      "-1.0613 -1.9441 -0.9596  0.5489 -0.9901  1.4414  0.1690  0.2575  0.1212 -1.8270\n",
      "-0.3826  1.5037  1.8267  0.5561  1.6445  0.1571 -1.3312 -1.0505 -1.0007 -0.4621\n",
      " 0.4973 -1.5067  1.7661 -0.3569 -0.1713 -0.5060  1.1233  0.4800 -0.0344 -0.4928\n",
      "[torch.FloatTensor of size 4x10]\n",
      "\n",
      "Variable containing:\n",
      " 30\n",
      " 21\n",
      "  4\n",
      " 17\n",
      " 24\n",
      "  1\n",
      " 14\n",
      "  3\n",
      " 24\n",
      "[torch.LongTensor of size 9]\n",
      "\n",
      "Variable containing:\n",
      "-0.8018 -0.7855  0.7877  0.0786  1.7053\n",
      "-0.9347 -0.9882  1.3801 -0.1173  0.9317\n",
      " 1.4666 -0.1028 -0.0097 -0.8420 -0.2067\n",
      " 1.6200  0.3436 -0.9112 -0.9952  0.7455\n",
      " 1.3210  1.1608  0.3457 -0.1136 -0.8910\n",
      "-0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
      "-0.2969 -0.0681 -0.2831 -0.4705 -1.7655\n",
      "-0.8890  0.2620  0.0302  0.0013 -1.3987\n",
      " 1.3210  1.1608  0.3457 -0.1136 -0.8910\n",
      "[torch.FloatTensor of size 9x5]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.8018 -0.7855  0.7877  0.0786  1.7053\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.9347 -0.9882  1.3801 -0.1173  0.9317\n",
      "\n",
      "(2 ,.,.) = \n",
      "  1.4666 -0.1028 -0.0097 -0.8420 -0.2067\n",
      "\n",
      "(3 ,.,.) = \n",
      "  1.6200  0.3436 -0.9112 -0.9952  0.7455\n",
      "\n",
      "(4 ,.,.) = \n",
      "  1.3210  1.1608  0.3457 -0.1136 -0.8910\n",
      "\n",
      "(5 ,.,.) = \n",
      " -0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
      "\n",
      "(6 ,.,.) = \n",
      " -0.2969 -0.0681 -0.2831 -0.4705 -1.7655\n",
      "\n",
      "(7 ,.,.) = \n",
      " -0.8890  0.2620  0.0302  0.0013 -1.3987\n",
      "\n",
      "(8 ,.,.) = \n",
      "  1.3210  1.1608  0.3457 -0.1136 -0.8910\n",
      "[torch.FloatTensor of size 9x1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = nn.Embedding(len(word_to_ix), 5)\n",
    "char_embeddings = nn.Embedding(len(char_to_ix), 5)\n",
    "print(prepare_sequence(['Everybody', 'read', 'that', 'book'], word_to_ix))\n",
    "embeds = word_embeddings(prepare_sequence(['Everybody', 'read', 'that', 'book'], word_to_ix))\n",
    "print(embeds)\n",
    "print(torch.cat((embeds, torch.randn(4, 5)), 1))\n",
    "prepped_chars = prepare_sequence('Everybody', char_to_ix)\n",
    "print(prepped_chars)\n",
    "char_embeds = char_embeddings(prepped_chars)\n",
    "print(char_embeds)\n",
    "print(char_embeds.view(len('Everybody'),1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(CharEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(len(string.ascii_letters + ' '), embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, word, hidden):\n",
    "        embeds = self.embeddings(word)\n",
    "        return self.lstm(embeds.view(len(word), 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0694  0.2588  0.2725 -0.0762  0.0271  0.1901\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1410 -0.0108  0.0867 -0.0295  0.0695  0.0723\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.0600 -0.0699  0.3287 -0.2876  0.1991  0.0357\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.1751 -0.1662  0.2829 -0.1596  0.0308  0.0775\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.2352 -0.0011  0.3157 -0.1436  0.0696  0.1769\n",
      "\n",
      "(5 ,.,.) = \n",
      "  0.0229  0.2280  0.3494 -0.0650  0.1643  0.1937\n",
      "\n",
      "(6 ,.,.) = \n",
      "  0.2333  0.1481  0.1651  0.2835  0.1776  0.2084\n",
      "\n",
      "(7 ,.,.) = \n",
      "  0.0856  0.0761  0.3421  0.1467  0.1772  0.2845\n",
      "\n",
      "(8 ,.,.) = \n",
      " -0.0732  0.1365  0.3774 -0.0013  0.1512  0.2672\n",
      "[torch.FloatTensor of size 9x1x6]\n",
      ", (Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0732  0.1365  0.3774 -0.0013  0.1512  0.2672\n",
      "[torch.FloatTensor of size 1x1x6]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1613  0.2766  0.6521 -0.0031  0.5743  0.6119\n",
      "[torch.FloatTensor of size 1x1x6]\n",
      "))\n",
      "Variable containing:\n",
      " 0.0694  0.2588  0.2725 -0.0762  0.0271  0.1901\n",
      " 0.1410 -0.0108  0.0867 -0.0295  0.0695  0.0723\n",
      "-0.0600 -0.0699  0.3287 -0.2876  0.1991  0.0357\n",
      "-0.1751 -0.1662  0.2829 -0.1596  0.0308  0.0775\n",
      "-0.2352 -0.0011  0.3157 -0.1436  0.0696  0.1769\n",
      " 0.0229  0.2280  0.3494 -0.0650  0.1643  0.1937\n",
      " 0.2333  0.1481  0.1651  0.2835  0.1776  0.2084\n",
      " 0.0856  0.0761  0.3421  0.1467  0.1772  0.2845\n",
      "-0.0732  0.1365  0.3774 -0.0013  0.1512  0.2672\n",
      "[torch.FloatTensor of size 9x6]\n",
      "\n",
      "Variable containing:\n",
      "-0.0732\n",
      " 0.1365\n",
      " 0.3774\n",
      "-0.0013\n",
      " 0.1512\n",
      " 0.2672\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "Variable containing:\n",
      "-0.0732  0.1365  0.3774 -0.0013  0.1512  0.2672\n",
      "-0.0732  0.1365  0.3774 -0.0013  0.1512  0.2672\n",
      "[torch.FloatTensor of size 2x6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ce = CharEncoder(6, 6)\n",
    "ce_out = ce(prepare_sequence('Everybody', char_to_ix), ce.init_hidden())\n",
    "print(ce_out)\n",
    "print(ce_out[0].view(len('Everybody'), -1))\n",
    "print(ce_out[1][0].view(6))  # hidden state\n",
    "print(torch.cat((ce_out[1][0].view(6), ce_out[1][0].view(6))).view(-1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim + char_hidden_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence, char_hidden_states):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "                \n",
    "        # augment the word embeddings\n",
    "        aug = torch.cat(char_hidden_states).view(-1, self.char_hidden_dim)\n",
    "        input = torch.cat((embeds, aug), 1)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            input.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag(sentence, char_model, tagger):\n",
    "    # get the char hidden states for each word\n",
    "    char_hidden_states = []\n",
    "    for word in sentence:\n",
    "        word_in = prepare_sequence(word, char_to_ix)\n",
    "        _, (hidden, _) = char_model(word_in, char_model.init_hidden())\n",
    "        char_hidden_states.append(hidden.view(char_model.hidden_dim))\n",
    "        \n",
    "    # prepare the inputs\n",
    "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "    \n",
    "    # run forward pass for the tagger\n",
    "    return tagger(sentence_in, char_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.8922 -1.4439 -1.0378\n",
      "-0.9071 -1.4206 -1.0364\n",
      "-0.9318 -1.3157 -1.0851\n",
      "-0.8566 -1.2834 -1.2097\n",
      "-0.9763 -1.3206 -1.0319\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "['DET', 'DET', 'DET', 'DET', 'DET']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fbaa780aae56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# calculate loss, gradients and update the params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mchar_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtag_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/pytorch-practice-3.6.2/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/pytorch-practice-3.6.2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/pytorch-practice-3.6.2/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/pytorch-practice-3.6.2/lib/python3.6/site-packages/torch/autograd/_functions/basic_ops.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_unexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_unexpand_or_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "char_model = CharEncoder(EMBEDDING_DIM, HIDDEN_DIM)\n",
    "tagger = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "char_optimizer = optim.SGD(char_model.parameters(), lr=0.1)\n",
    "tag_optimizer = optim.SGD(tagger.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "tag_scores = tag(training_data[0][0], char_model, tagger)\n",
    "print(tag_scores)\n",
    "_, predicted = torch.max(tag_scores.data, 1)\n",
    "print([ix_to_tag[p] for p in predicted])\n",
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        # zero out gradients\n",
    "        char_model.zero_grad()\n",
    "        tagger.zero_grad()\n",
    "        \n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        tag_scores = tag(sentence, char_model, tagger)\n",
    "        \n",
    "        # calculate loss, gradients and update the params\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        char_optimizer.step()\n",
    "        tag_optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print(loss.data)\n",
    "            \n",
    "# See what the scores are after training\n",
    "tag_scores = tag(training_data[0][0], char_model, tagger)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print(tag_scores)\n",
    "_, predicted = torch.max(tag_scores.data, 1)\n",
    "print([ix_to_tag[p] for p in predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.1086 -1.3025 -0.9210\n",
      "-1.0946 -1.2369 -0.9807\n",
      "-1.1936 -1.3858 -0.8058\n",
      "-1.1100 -1.2606 -0.9494\n",
      "-1.1319 -1.2421 -0.9447\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "['V', 'V', 'V', 'V', 'V']\n",
      "\n",
      " 1.0753\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 0.2795\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-02 *\n",
      "  4.5409\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.0647 -3.0983 -4.0455\n",
      "-4.5395 -0.0112 -7.5848\n",
      "-4.1240 -7.4205 -0.0169\n",
      "-0.0131 -4.7049 -5.5385\n",
      "-4.3413 -0.0139 -7.1342\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "[{0: 'DET', 1: 'NN', 2: 'V'}, {0: 'DET', 1: 'NN', 2: 'V'}, {0: 'DET', 1: 'NN', 2: 'V'}, {0: 'DET', 1: 'NN', 2: 'V'}, {0: 'DET', 1: 'NN', 2: 'V'}]\n"
     ]
    }
   ],
   "source": [
    "class LSTMTaggerExample(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTaggerExample, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores\n",
    "\n",
    "model = LSTMTaggerExample(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)\n",
    "_, predicted = torch.max(tag_scores.data, 1)\n",
    "print([ix_to_tag[p] for p in predicted])\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss.data)\n",
    "\n",
    "# See what the scores are after training\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print(tag_scores)\n",
    "_, predicted = torch.max(tag_scores.data, 1)\n",
    "print([ix_to_tag for p in predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerCombined(nn.Module):\n",
    "\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, word_hidden_dim, char_hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTaggerCombined, self).__init__()\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.char_embeddings = nn.Embedding(len(string.ascii_letters + ' '), char_embedding_dim)\n",
    "\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
    "        # The LSTM takes word embeddings and char representation as inputs, \n",
    "        # and outputs hidden states with dimensionality hidden_dim.\n",
    "        self.word_lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(word_hidden_dim, tagset_size)\n",
    "\n",
    "    def init_char_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.char_hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.char_hidden_dim)))\n",
    "    \n",
    "    def init_word_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.word_hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.word_hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence, word_hidden, char_hidden):\n",
    "        # get the char hidden states for each word\n",
    "        char_hidden_states = []\n",
    "        for word in sentence:\n",
    "            word_in = prepare_sequence(word, char_to_ix)\n",
    "            embeds = self.char_embeddings(word_in)\n",
    "            _, (hidden, _) = self.char_lstm(embeds.view(len(word), 1, -1), char_hidden)\n",
    "            char_hidden_states.append(hidden.view(self.char_hidden_dim))\n",
    "\n",
    "        # prepare the inputs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        embeds = self.word_embeddings(sentence_in)\n",
    "                \n",
    "        # augment the word embeddings\n",
    "        aug = torch.cat(char_hidden_states).view(-1, self.char_hidden_dim)\n",
    "        input = torch.cat((embeds, aug), 1)\n",
    "        lstm_out, _ = self.word_lstm(\n",
    "            input.view(len(sentence), 1, -1), word_hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.5590 -1.1455 -0.7517\n",
      "-1.7298 -1.0822 -0.7260\n",
      "-1.9817 -1.2273 -0.5637\n",
      "-1.9563 -1.1564 -0.6088\n",
      "-1.7868 -1.1616 -0.6549\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "['V', 'V', 'V', 'V', 'V']\n",
      "\n",
      " 1.2574\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 0.1827\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e-02 *\n",
      "  3.1274\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.0159 -4.2114 -7.0014\n",
      "-5.8940 -0.0049 -6.1528\n",
      "-6.5028 -4.5698 -0.0119\n",
      "-0.0077 -4.9708 -7.1924\n",
      "-5.0401 -0.0106 -5.4953\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerCombined(EMBEDDING_DIM, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "tag_scores = model(training_data[0][0], model.init_word_hidden(), model.init_char_hidden())\n",
    "print(tag_scores)\n",
    "_, predicted = torch.max(tag_scores.data, 1)\n",
    "print([ix_to_tag[p] for p in predicted])\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get the targets ready\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence, model.init_word_hidden(), model.init_char_hidden())\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss.data)\n",
    "\n",
    "# See what the scores are after training\n",
    "tag_scores = model(training_data[0][0], model.init_word_hidden(), model.init_char_hidden())\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print(tag_scores)\n",
    "_, predicted = torch.max(tag_scores.data, 1)\n",
    "print([ix_to_tag[p] for p in predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
